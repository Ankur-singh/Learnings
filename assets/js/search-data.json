{
  
    
        "post0": {
            "title": "Distributions",
            "content": "In this blog, we will discuss all about distributions from Data Science &amp; EDA perspective. We will be using &quot;diamonds&quot; dataset from seaborn library. . import seaborn as sns import pandas as pd data = sns.load_dataset(&#39;diamonds&#39;) data.head() . carat cut color clarity depth table price x y z . 0 0.23 | Ideal | E | SI2 | 61.5 | 55.0 | 326 | 3.95 | 3.98 | 2.43 | . 1 0.21 | Premium | E | SI1 | 59.8 | 61.0 | 326 | 3.89 | 3.84 | 2.31 | . 2 0.23 | Good | E | VS1 | 56.9 | 65.0 | 327 | 4.05 | 4.07 | 2.31 | . 3 0.29 | Premium | I | VS2 | 62.4 | 58.0 | 334 | 4.20 | 4.23 | 2.63 | . 4 0.31 | Good | J | SI2 | 63.3 | 58.0 | 335 | 4.34 | 4.35 | 2.75 | . Dataset &amp; basic EDA . Every dataset consists of samples (or records) and for every sample we record multiple features. Here, every sample is a diamond &amp; for every diamond we have recorded carat, cut, color, clarity, depth, table, price, x, y &amp; z features as we can see in the table above. . Its a good practice to check the shape of the dataset, missing values &amp; unique values, even before we start the exploration process. . data.shape . (53940, 10) . Wow! we have a lot of data . . . . data.isnull().sum() . carat 0 cut 0 color 0 clarity 0 depth 0 table 0 price 0 x 0 y 0 z 0 dtype: int64 . We don&#39;t have any missing values. This is good news. . Now lets look at unique values . . . . data.nunique() . carat 273 cut 5 color 7 clarity 8 depth 184 table 127 price 11602 x 554 y 552 z 375 dtype: int64 . A feature can be either categorical or continuous. . Categorical Features: Finite &amp; Distinct values. Here, cut, color and clarity are categorical features. | Continuous Features: Infinite &amp; continuous values. | . Its extremely important to identify your categorical &amp; continuous features because they require different treatment when it comes to analysis &amp; pre-processing. . Frequency Table . Now that we know our categorical &amp; continuous features, lets start the exploration . . . we will start with cut feature . data.cut.values . [&#39;Ideal&#39;, &#39;Premium&#39;, &#39;Good&#39;, &#39;Premium&#39;, &#39;Good&#39;, ..., &#39;Ideal&#39;, &#39;Good&#39;, &#39;Very Good&#39;, &#39;Premium&#39;, &#39;Ideal&#39;] Length: 53940 Categories (5, object): [&#39;Ideal&#39;, &#39;Premium&#39;, &#39;Very Good&#39;, &#39;Good&#39;, &#39;Fair&#39;] . There are 53940 values in total &amp; 5 unique values. But this is not very useful. So lets create a frequency table out of it . data.cut.value_counts() . Ideal 21551 Premium 13791 Very Good 12082 Good 4906 Fair 1610 Name: cut, dtype: int64 . Now we can clearly see how many diamonds we have for each possible value of cut. This is much better than randomly throwing a list of ~54K values at someone. . Similarly, lets explore other columns . . . . data.color.value_counts() . G 11292 E 9797 F 9542 H 8304 D 6775 I 5422 J 2808 Name: color, dtype: int64 . data.clarity.value_counts() . SI1 13065 VS2 12258 SI2 9194 VS1 8171 VVS2 5066 VVS1 3655 IF 1790 I1 741 Name: clarity, dtype: int64 . So far, we have very few unique values. And still we are not able to make such sense out of these frequency tables. Imagine, if we have a lot of unique values then it will quickly become unmanageable. . Bar Plot . Remember, humans are visual creatures. So lets create a bar plot describing above table. . tmp = data.clarity.value_counts() p = sns.barplot(x=tmp.index, y=tmp.values) p.set(xlabel=&#39;Clarity&#39;, ylabel=&#39;Count&#39;); . Again, this is much better. We can clearly see people prefer VS2 &amp; SI1 over other category of clarity. . Using these three things (data, frequency table, &amp; bar plot) we can answer a lot of interesting question about our data. For example: . What is most &amp; least frequent category? | How many more people bought VS2 compared to VS1? | . Things get really interesting once you start repeating the same process for other categorical features as well. Not just other categories, you can also repeat the process for any subset of data. . Comparing Bar plots . Say We want to see how clarity changes for Ideal cut of diamonds . ideal = data[data.cut == &quot;Ideal&quot;] ideal.shape . (21551, 10) . tmp = ideal.clarity.value_counts() p = sns.barplot(x=tmp.index, y=tmp.values) p.set(xlabel=&#39;Clarity&#39;, ylabel=&#39;Count&#39;); . Now lets, see how clarity is distributed for Good cut of diamonds. . good = data[data.cut == &quot;Good&quot;] good.shape . (4906, 10) . tmp = good.clarity.value_counts() p = sns.barplot(x=tmp.index, y=tmp.values) p.set(xlabel=&#39;Clarity&#39;, ylabel=&#39;Count&#39;); . As you can see most other categories are same but in Good cut diamonds, SI1 is the most popular category where as in Ideal cut diamonds, VS2 is the most popular category. . Lets compare both these distributions by plotting them together . . . . tmp1 = pd.DataFrame(ideal.clarity.value_counts()) tmp1.reset_index(inplace=True) tmp1[&#39;cut&#39;] = &#39;Ideal&#39; tmp1.columns = [&#39;clarity&#39;, &#39;count&#39;, &#39;cut&#39;] tmp2 = pd.DataFrame(good.clarity.value_counts()) tmp2.reset_index(inplace=True) tmp2[&#39;cut&#39;] = &#39;Good&#39; tmp2.columns = [&#39;clarity&#39;, &#39;count&#39;, &#39;cut&#39;] tmp = pd.concat([tmp1, tmp2]) sns.barplot(x=tmp.clarity, y=tmp[&#39;count&#39;], hue=tmp.cut); . . Huh 🤔, its pretty obvious that we have many more Ideal diamonds than Good diamonds. Its not a fair comparison, because of bigger sample size, Ideal diamonds are completely overshadowing the Good diamonds. . For a fair comparison instead of comparing absolute frequency values, we should compare the percentage values. . Probability Mass Functions (PMFs) . Count Plots are not good for comparing distributions. Hence we need to normalize them. These plots are know as Probability Mass functions (PMFs). . Lets see how we can normalize our frequency tables to make PMF plots . ideal.clarity.value_counts()/len(ideal) . VS2 0.235302 SI1 0.198691 VS1 0.166535 VVS2 0.120922 SI2 0.120551 VVS1 0.094984 IF 0.056239 I1 0.006775 Name: clarity, dtype: float64 . pretty simple, right? If you just divide your absolute values by total number of samples (i.e length of the corresponding dataframe) then you will get percentage values. . The above table shows that 23.5% of Ideal cut diamond has VS2 clarity. . Lets now try to plot these percentage values. . tmp1 = pd.DataFrame(ideal.clarity.value_counts()/len(ideal)) tmp1.reset_index(inplace=True) tmp1[&#39;cut&#39;] = &#39;Ideal&#39; tmp1.columns = [&#39;clarity&#39;, &#39;prob&#39;, &#39;cut&#39;] tmp2 = pd.DataFrame(good.clarity.value_counts()/len(good)) tmp2.reset_index(inplace=True) tmp2[&#39;cut&#39;] = &#39;Good&#39; tmp2.columns = [&#39;clarity&#39;, &#39;prob&#39;, &#39;cut&#39;] tmp = pd.concat([tmp1, tmp2]) sns.barplot(x=tmp.clarity, y=tmp.prob, hue=tmp.cut); . . WOW! this is huge difference. Take a minute to compare both the plot. Can you spot the difference? Hint: look at y-axis. . Clearly, SI1, SI2 and I1 are more popular in Good diamonds than Ideal diamonds. This was not visible when we tried to compare them using absolute values. . Important: Whenever comparing two or more distributions make sure you normalize them before comparison. Wonderful, now we know how to compare two distributions using bar plots. But we cannot use bar plot if we have a lot unique values or for continuous features. . For example, lets try to make a bar plot for carat feature . tmp = data.carat.value_counts() sns.barplot(x=tmp.index, y=tmp.values); . The labels on the x-axis are not at all readable. There are so many small values that are hardly visible. Also, a lot of unique values make it difficult to understand the plot. We will see, how we can use histograms to alleviate this problem. . Histogram . Instead of creating a separate bar for every unique value, we create bins for small ranges of values and put all the values in that range inside the bin. . sns.histplot(data=data, x=&#39;carat&#39;, bins=50); . But histograms are not perfect. Changing the number of bins can result in drastically different plot. It can also obscure some meaningful insight from the data. Here is example . sns.histplot(data=data, x=&#39;carat&#39;, bins=100); . The plot will 100 bins looks very different from the one with 50 bins. . Histograms works great when you well defined bins. For example: all age &lt;18 is minor, &gt;18 &amp; &lt;60 is adult, and &gt;60 is senior citizen. In other cases, try to avoid them. . Probability Density Functions (PDFs) . For simplicity, you can think of PDFs are histograms with a lot of bins &amp; then some smoothing to reduce the impact of missing values &amp; noise. . Technical Stuff (not important) . Technically, PDFs are derivatives of Cumulative Distribution Functions (CDFs). CDFs are nothing but cumulative sum of PMFs. . In physics, Density of a substance is defined as mass per unit volume. No matter how the mass or volume changes, their ratio (i.e density) will always remain constant. Similarly, no matter how many data points you select from the distribution and the absolute values of these data points, the PDF will (more or less) look the same. . Hence, PDF is a very apt name. Density is the way you summaries substances &amp; also distributions. PDF is the standard way to describe any distribution. . . PDFs are very expensive to compute &amp; often times don&#39;t have real solutions. Kernel density estimation (KDE) is an algorithm that takes a sample and finds an appropriately smooth PDF that fits the data. You can read details at wikipedia. . Don&#39;t worry about all these technical details, you can easily make KDE plot using seaborn. . sns.kdeplot(data=data, x=&#39;carat&#39;); . In summary, you can always use KDE plots for continuous features &amp; bar plot for categorical features. If you have well defines bins for continuous features then you can also use histograms. . Thinks to look for in distributions . Often, we want to summarize the distribution with a few descriptive statistics. Some of the characteristics we might want to look for, are: . Central tendency: Do the values tend to cluster around a particular point? | Modes: Is there more than one cluster? | Spread: How much variability is there in the values? | Tails: How quickly do the probabilities drop off as we move away from the modes? Skewness is a property that describes the shape of a distribution. If the distribution is symmetric around its central tendency, it is unskewed. If the values extend farther to the right, it is right skewed and if the values extend left, it is left skewed. | Outliers: Are there extreme values far from the modes? The best way to handle outliers depends on “domain knowledge”; that is, information about where the data come from and what they mean. And it depends on what analysis you are planning to perform | . Statistics designed to answer these questions are called summary statistics. . Along with these plot, you can used other stats about the distribution like mean, median, mode, min, max, 25 &amp; 75 percentile, Inter Quartile Range (IQR), etc to better understand &amp; describe your distribution. . Model Estimation . There are two types of distributions: empirical and analytic. . The distributions we have used so far are called empirical distributions because they are based on empirical observations, which are necessarily finite samples. . On the contrary, analytic distributions are described by analytic mathematical function. Here are some popular analytic distributions : normal/guassian, log-normal, exponential, binomial, poisson, etc. Here is a good blog discussing some of these distributions in detail. . It turns out that many things we measure in the world have distributions that are well approximated by analytic distributions, so these distributions are sometimes good models for the real world. By &quot;Model&quot;, I mean a simplified description of the world that is accurate enough for its intended purpose. . Why model/estimate? . Like all models, analytic distributions are abstractions, which means they leave out details that are considered irrelevant. For example, an observed distribution might have measurement errors or quirks that are specific to the sample; analytic models smooth out these idiosyncrasies. | Analytic models are also a form of data compression. When a model fits a dataset well, a small set of parameters can summarize a large amount of data. For example, to describe a normal distribution, you only need two parameters: mean &amp; standard deviation. | A lot of study is already available for analytic distributions. For example, if you know your data follows normal distribution then you can easily tell the range in which 68.2% of your data will lay by looking at the image below. | . . Hope you had a wonderful time reading this blog and also, you learned something useful. Cheers 🎉 . Further Reading . Seaborn Distributions tutorial - Highly Recommended | Distributions in SciPy | .",
            "url": "https://ankur-singh.github.io/learnings/statistics/eda/2021/06/04/Distributions.html",
            "relUrl": "/statistics/eda/2021/06/04/Distributions.html",
            "date": " • Jun 4, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "How to - EDA",
            "content": "How to : EDA . The content of this blog was originally written as personal notes that I created after reading “Think Stats 2e”. Since then, I follow it every time I start working with a new dataset. Finally, I decided to turn this into a blog post for two reasons: . So, others can also benefit from it. | I can access it from anywhere. | Without ado, lets dive right in . . . . . Steps . These are the steps that I follow when performing EDA on some dataset. . . Loading and cleaning: . Whatever format the data is in, it usually takes some time and effort to read the data, clean and transform it, and check that everything made it through the translation process intact. Pandas make it super easy to load data from any major file format. Immediately after loading the data, I like to call df.head() and df.shape to see if everything was loaded correctly. Pandas makes cleaning data super easy &amp; FUN! But there are situations where you might have to use other tools as well. Don’t worry, python has plenty of them. . | Single variable explorations: . I usually start by examining one variable at a time, finding out what the variables mean, looking at distributions of the values, and choosing appropriate summary statistics. In pandas, you can simple call df.describe() to see summary stats for all the numerical columns. Often times, just by looking at the output, you can easily find a lot about the data. If you see something fishy, then thats the feature you should explore next. PS: People often struggle to figure out what to do next in EDA. Look at the output of df.describe(). Its a good starting point. . | Pair-wise explorations: . To identify possible relationships between variables, I look at tables and scatter plots, and compute correlations and linear fits. Scatter plots are great, but at times they can be misleading. Hence, its difficult to make a good scatter plot that is not misleading and might take some time getting use to. I use seaborn, matplotlib and pandas to make plots. One can easily find correlations between different features by calling df.corr(). In the beginning, I struggled to interpreting the relationship correctly. With some practice you can learn to understand them better. May be some other time, I might write a blog on it. For now, take this . . . keep visualizing your data. . | Multivariate analysis: . If there are apparent relationships between variables, I use multiple regression to add control variables and investigate more complex relationships. Both pearson’s correlation &amp; linear fit can only measure linear relationships. But most of the relationship in empirical data is complex &amp; non-linear. So, you need to be careful. Python has some amazing libraries for exploring linear &amp; multivariate relations like statsmodel, scipy, sklearn, etc. . | Estimation and hypothesis testing: . When reporting statistical results, it is important to answer three questions: How big is the effect? How much variability should we expect if we run the same measurement again on different sample? Is it possible that the apparent effect is due to chance? Model estimation is a great way to describe you data. With good estimates, you can learn a lot more about the data distribution with very few samples. This is the part where some domain expertise will take you a long way. Say you are working with health data. You need to know what is the range of “blood pressure” that is considered “High BP” and “Low BP”. Same goes for “blood sugar level”. Another thing to know it what common patterns to expect if someone has high BP. With some domain knowledge and common sense, you can easily come up with list of things (or constraints) that should be true/false for the data. These are called hypothesis &amp; the process of verifying these constraints is called hypothesis testing. . | Visualization: . Visualization is not a step as such. I have still written it because it should be used at every stage of EDA. During exploration, visualization is an important tool for finding possible relationships and effects. Then if an apparent effect holds up to scrutiny, visualization is an effective way to communicate results. I think visualization is more of an ART than science. So, one needs a lot of practice to get mastery over it. You cannot learn to do it from others. . | This is the 5 step process that I follow, and the tools that I use while doing EDA. Hope this blog was helpful. I will be discussing each of these steps, in depth, in my future blogs. So, stay tuned and thank you so much for reading. .",
            "url": "https://ankur-singh.github.io/learnings/markdown/2020/06/04/EDA.html",
            "relUrl": "/markdown/2020/06/04/EDA.html",
            "date": " • Jun 4, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://ankur-singh.github.io/learnings/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://ankur-singh.github.io/learnings/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://ankur-singh.github.io/learnings/_pages/about.html",
          "relUrl": "/_pages/about.html",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ankur-singh.github.io/learnings/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}